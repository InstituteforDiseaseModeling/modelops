# Dask Workspace Configuration for ModelOps
# Usage: mops workspace up --config examples/workspace.yaml --env dev
#
# Note: Stack names are automatically generated based on environment
# Stack pattern: modelops-workspace-{env}
# Namespace pattern: modelops-dask-{env}
#
# Process vs Thread Configuration:
# - For simulation workloads (pure Python): Use processes=2, threads=1
# - For data science (NumPy/Pandas): Use processes=1, threads=4
# See docs/dask-configuration.md for detailed guidance
#
# Executor Type Configuration (MODELOPS_EXECUTOR_TYPE):
# - isolated_warm: Warm subprocess pool, fast but reuses processes (default)
# - direct: No subprocess pool, runs in worker process (faster but no isolation)
# - cold: Fresh process per task, slowest but maximum isolation (for debugging C++ state issues)
# See DISABLE_THREADING_AND_WARM_PROCESSES.md and CPP_STATE_LEAKAGE_FIX.md

apiVersion: modelops/v1
kind: Workspace
metadata:
  name: dev-workspace
  # Namespace is auto-generated based on environment: modelops-dask-{env}

spec:
  # Dask Scheduler Configuration
  scheduler:
    image: ghcr.io/institutefordiseasemodeling/modelops-dask-scheduler:latest
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "2Gi"
        cpu: "1"
    # No node selector - schedule on any available worker nodes
    # Environment variables for scheduler
    env:
      - name: DASK_SCHEDULER__DASHBOARD__ENABLED
        value: "true"
      - name: DASK_SCHEDULER__PRELOAD
        value: "dask_kubernetes"

  # Dask Workers Configuration
  workers:
    replicas: 4  # Initial replicas (ignored when autoscaling is enabled)
    image: ghcr.io/institutefordiseasemodeling/modelops-dask-worker:latest
    resources:
      requests:
        memory: "16Gi"
        cpu: "2"
      limits:
        memory: "16Gi"
        cpu: "2"
    # No node selector - schedule on any available worker nodes
    # Process and thread configuration
    # For simulation workloads (pure Python), use more processes and fewer threads
    # to bypass GIL limitations. For NumPy/Pandas workloads, use more threads.
    # NOTE: Increased from 2 to 4 processes to prevent aggregation deadlock
    # With 200 replicates per aggregation, we need extra threads to avoid starvation
    processes: 4  # Number of worker processes per pod (--nprocs)
    threads: 1    # Number of threads per worker process (--nthreads)
    # Environment variables for workers
    env:
      # Memory management
      - name: DASK_WORKER__MEMORY__TARGET
        value: "0.90"  # Spill to disk at 90% memory
      - name: DASK_WORKER__MEMORY__SPILL
        value: "0.95"  # Spill aggressively at 95%
      - name: DASK_WORKER__MEMORY__PAUSE
        value: "0.98"  # Pause execution at 98%

      # Executor type (uncomment ONE of these):
      # - name: MODELOPS_EXECUTOR_TYPE
      #   value: "isolated_warm"  # Default: warm subprocess pool (fast, reuses processes)
      # - name: MODELOPS_EXECUTOR_TYPE
      #   value: "cold"  # Cold: fresh process per task (slow, maximum isolation for debugging)

      # Optional: Force fresh venv per task (slowest, most isolated)
      # - name: MODELOPS_FORCE_FRESH_VENV
      #   value: "true"

  # Autoscaling Configuration
  # When enabled, the number of workers scales based on CPU utilization
  autoscaling:
    enabled: true  # Enable/disable autoscaling (default: true)
    min_workers: 2  # Minimum number of worker pods
    max_workers: 10  # Maximum number of worker pods
    target_cpu: 70  # Target CPU utilization percentage
    scale_down_delay: 300  # Seconds to wait before scaling down

  # Optional: Authentication for private registries
  # Uncomment to use GitHub Container Registry with PAT
  # imagePullSecrets:
  #   - name: ghcr-creds
  
  # Optional: Tolerations for tainted nodes
  # Uncomment if your CPU nodes have taints
  # tolerations:
  #   - key: "modelops.io/role"
  #     operator: "Equal"
  #     value: "cpu"
  #     effect: "NoSchedule"
