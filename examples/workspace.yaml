# Dask Workspace Configuration for ModelOps
# Usage: mops workspace up --config examples/workspace.yaml --env dev
#
# Note: Stack names are automatically generated based on environment
# Stack pattern: modelops-workspace-{env}
# Namespace pattern: modelops-dask-{env}
#
# Process vs Thread Configuration:
# - For simulation workloads (pure Python): Use processes=2, threads=1
# - For data science (NumPy/Pandas): Use processes=1, threads=4
# See docs/dask-configuration.md for detailed guidance

apiVersion: modelops/v1
kind: Workspace
metadata:
  name: dev-workspace
  # Namespace is auto-generated based on environment: modelops-dask-{env}

spec:
  # Dask Scheduler Configuration
  scheduler:
    image: ghcr.io/institutefordiseasemodeling/modelops-dask-scheduler:latest
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "2Gi"
        cpu: "1"
    # No node selector - schedule on any available worker nodes
    # Environment variables for scheduler
    env:
      - name: DASK_SCHEDULER__DASHBOARD__ENABLED
        value: "true"
      - name: DASK_SCHEDULER__PRELOAD
        value: "dask_kubernetes"

  # Dask Workers Configuration
  workers:
    replicas: 4  # Initial replicas (ignored when autoscaling is enabled)
    image: ghcr.io/institutefordiseasemodeling/modelops-dask-worker:latest
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "4Gi"
        cpu: "2"
    # No node selector - schedule on any available worker nodes
    # Process and thread configuration
    # For simulation workloads (pure Python), use more processes and fewer threads
    # to bypass GIL limitations. For NumPy/Pandas workloads, use more threads.
    processes: 2  # Number of worker processes per pod (--nprocs)
    threads: 1    # Number of threads per worker process (--nthreads)
    # Environment variables for workers
    env:
      - name: DASK_WORKER__MEMORY__TARGET
        value: "0.90"  # Spill to disk at 90% memory
      - name: DASK_WORKER__MEMORY__SPILL
        value: "0.95"  # Spill aggressively at 95%
      - name: DASK_WORKER__MEMORY__PAUSE
        value: "0.98"  # Pause execution at 98%

  # Autoscaling Configuration
  # When enabled, the number of workers scales based on CPU utilization
  autoscaling:
    enabled: true  # Enable/disable autoscaling (default: true)
    min_workers: 2  # Minimum number of worker pods
    max_workers: 10  # Maximum number of worker pods
    target_cpu: 70  # Target CPU utilization percentage
    scale_down_delay: 300  # Seconds to wait before scaling down

  # Optional: Authentication for private registries
  # Uncomment to use GitHub Container Registry with PAT
  # imagePullSecrets:
  #   - name: ghcr-creds
  
  # Optional: Tolerations for tainted nodes
  # Uncomment if your CPU nodes have taints
  # tolerations:
  #   - key: "modelops.io/role"
  #     operator: "Equal"
  #     value: "cpu"
  #     effect: "NoSchedule"
